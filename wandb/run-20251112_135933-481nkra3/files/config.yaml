_current_progress_remaining:
    value: 1
_custom_logger:
    value: "False"
_episode_num:
    value: 0
_last_episode_starts:
    value: '[ True]'
_last_obs:
    value: '[[0.        1.        0.        0.        0.        0.        0.6666667]]'
_last_original_obs:
    value: None
_logger:
    value: <stable_baselines3.common.logger.Logger object at 0x16871e270>
_n_calls:
    value: 0
_n_updates:
    value: 0
_num_timesteps_at_start:
    value: 0
_stats_window_size:
    value: 100
_total_timesteps:
    value: 1000000
_vec_normalize_env:
    value: None
_wandb:
    value:
        cli_version: 0.22.2
        e:
            tjs2bv38qc7o2akgha7m7rcb1wezpnqb:
                apple:
                    ecpuCores: 4
                    gpuCores: 20
                    memoryGb: 48
                    name: Apple M4 Pro
                    pcpuCores: 10
                    ramTotalBytes: "51539607552"
                codePath: main.py
                codePathLocal: main.py
                cpu_count: 14
                cpu_count_logical: 14
                disk:
                    /:
                        total: "494384795648"
                        used: "158900420608"
                email: ghuckerby06@gmail.com
                executable: /Users/glynhuckerby/Computer Science/Dissertation/f1-rl-strategy/.venv/bin/python
                git:
                    commit: adf6c194736bc5eb5133e0d4e4a9828cae9c1d58
                    remote: https://github.com/ghuckerby/f1-rl-strategy.git
                host: Glyns-MacBook-Pro.local
                memory:
                    total: "51539607552"
                os: macOS-15.6.1-arm64-arm-64bit-Mach-O
                program: /Users/glynhuckerby/Computer Science/Dissertation/f1-rl-strategy/main.py
                python: CPython 3.13.5
                root: /Users/glynhuckerby/Computer Science/Dissertation/f1-rl-strategy
                startedAt: "2025-11-12T13:59:33.903435Z"
                writerId: tjs2bv38qc7o2akgha7m7rcb1wezpnqb
        m: []
        python_version: 3.13.5
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 2
                - 3
                - 13
                - 22
                - 35
            "4": 3.13.5
            "5": 0.22.2
            "10":
                - 20
            "12": 0.22.2
            "13": darwin-arm64
action_noise:
    value: None
action_space:
    value: Discrete(4)
algo:
    value: DQN
batch_norm_stats:
    value: '[]'
batch_norm_stats_target:
    value: '[]'
batch_size:
    value: 256
buffer_size:
    value: 500000
device:
    value: cpu
env:
    value: <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x11fff4830>
ep_info_buffer:
    value: deque([], maxlen=100)
ep_success_buffer:
    value: deque([], maxlen=100)
exploration_final_eps:
    value: 0.05
exploration_fraction:
    value: 0.05
exploration_initial_eps:
    value: 1
exploration_rate:
    value: 0
exploration_schedule:
    value: LinearSchedule(start=1.0, end=0.05, end_fraction=0.05)
gamma:
    value: 0.99
gradient_steps:
    value: 1
learning_rate:
    value: 0.0002
learning_starts:
    value: 50000
lr_schedule:
    value: FloatSchedule(ConstantSchedule(val=0.0002))
max_grad_norm:
    value: 10
n_envs:
    value: 1
n_steps:
    value: 1
num_timesteps:
    value: 0
observation_space:
    value: Box(0.0, 1.0, (7,), float32)
optimize_memory_usage:
    value: "False"
policy:
    value: |-
        DQNPolicy(
          (q_net): QNetwork(
            (features_extractor): FlattenExtractor(
              (flatten): Flatten(start_dim=1, end_dim=-1)
            )
            (q_net): Sequential(
              (0): Linear(in_features=7, out_features=64, bias=True)
              (1): ReLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): ReLU()
              (4): Linear(in_features=64, out_features=4, bias=True)
            )
          )
          (q_net_target): QNetwork(
            (features_extractor): FlattenExtractor(
              (flatten): Flatten(start_dim=1, end_dim=-1)
            )
            (q_net): Sequential(
              (0): Linear(in_features=7, out_features=64, bias=True)
              (1): ReLU()
              (2): Linear(in_features=64, out_features=64, bias=True)
              (3): ReLU()
              (4): Linear(in_features=64, out_features=4, bias=True)
            )
          )
        )
policy_class:
    value: <class 'stable_baselines3.dqn.policies.DQNPolicy'>
policy_kwargs:
    value: '{}'
q_net:
    value: |-
        QNetwork(
          (features_extractor): FlattenExtractor(
            (flatten): Flatten(start_dim=1, end_dim=-1)
          )
          (q_net): Sequential(
            (0): Linear(in_features=7, out_features=64, bias=True)
            (1): ReLU()
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): ReLU()
            (4): Linear(in_features=64, out_features=4, bias=True)
          )
        )
q_net_target:
    value: |-
        QNetwork(
          (features_extractor): FlattenExtractor(
            (flatten): Flatten(start_dim=1, end_dim=-1)
          )
          (q_net): Sequential(
            (0): Linear(in_features=7, out_features=64, bias=True)
            (1): ReLU()
            (2): Linear(in_features=64, out_features=64, bias=True)
            (3): ReLU()
            (4): Linear(in_features=64, out_features=4, bias=True)
          )
        )
replay_buffer:
    value: <stable_baselines3.common.buffers.ReplayBuffer object at 0x11fff4c20>
replay_buffer_class:
    value: <class 'stable_baselines3.common.buffers.ReplayBuffer'>
replay_buffer_kwargs:
    value: '{}'
sde_sample_freq:
    value: -1
seed:
    value: None
start_time:
    value: 1762955975213654000
target_update_interval:
    value: 1000
tau:
    value: 1
tensorboard_log:
    value: f1_gym/deterministic/dqn_logs/tensorboard/481nkra3
train_freq:
    value: 'TrainFreq(frequency=4, unit=<TrainFrequencyUnit.STEP: ''step''>)'
use_sde:
    value: "False"
use_sde_at_warmup:
    value: "False"
verbose:
    value: 1
